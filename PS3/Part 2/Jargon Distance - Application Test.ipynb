{
 "metadata": {
  "name": "",
  "signature": "sha256:8be76254aa085e05220759850a043c42a39d4d94ad29c63ea9b2bc9e24ba04df"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Import required packages\n",
      "\n",
      "import math\n",
      "import time\n",
      "import csv\n",
      "import nltk\n",
      "import numpy as np\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import RegexpTokenizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_time = time.time() # Start timer\n",
      "\n",
      "stopset = [\"all\",\"just\",\"being\",\"over\",\"both\",\"through\",\"yourselves\",\"its\",\"before\",\"herself\",\"had\",\"should\",\"to\",\"only\",\"under\",\n",
      "           \"ours\",\"has\",\"do\",\"them\",\"his\",\"very\",\"they\",\"not\",\"during\",\"now\",\"him\",\"nor\",\"did\",\"this\",\"she\",\"each\",\"further\",\"where\",\n",
      "           \"few\",\"because\",\"doing\",\"some\",\"are\",\"our\",\"ourselves\",\"out\",\"what\",\"for\",\"while\",\"does\",\"above\",\"between\",\"t\",\"be\",\"we\",\"who\",\n",
      "           \"were\",\"here\",\"hers\",\"by\",\"on\",\"about\",\"of\",\"against\",\"s\",\"or\",\"own\",\"into\",\"yourself\",\"down\",\"your\",\"from\",\"her\",\"their\",\n",
      "           \"there\",\"been\",\"whom\",\"too\",\"themselves\",\"was\",\"until\",\"more\",\"himself\",\"that\",\"but\",\"don\",\"with\",\"than\",\"those\",\"he\",\"me\",\n",
      "           \"myself\",\"these\",\"up\",\"will\",\"below\",\"can\",\"theirs\",\"my\",\"and\",\"then\",\"is\",\"am\",\"it\",\"an\",\"as\",\"itself\",\"at\",\"have\",\"in\",\n",
      "           \"any\",\"if\",\"again\",\"no\",\"when\",\"same\",\"how\",\"other\",\"which\",\"you\",\"after\",\"most\",\"such\",\"why\",\"a\",\"off\",\"i\",\"yours\",\"so\",\n",
      "           \"the\",\"having\",\"once\"]\n",
      "\n",
      "tokenizer = RegexpTokenizer(r'\\w+') # Initialize a tokenizer\n",
      "\n",
      "abstracts = open('abstracts.txt', 'r').readlines()\n",
      "abstracts = [abstract.strip().split('\\t') for abstract in abstracts]\n",
      "\n",
      "final = {}\n",
      "for key in abstracts:\n",
      "    x = tokenizer.tokenize(key[1]) # Tokenize words\n",
      "    x = [w.lower() for w in x if not w in stopset] # Convert words to lower case and remove words in stop list\n",
      "    final[key[0]] = x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "groups = open('groups.txt', 'r').readlines()\n",
      "groups = [group.strip().split('\\t') for group in groups[1:]]\n",
      "\n",
      "# Create clusters\n",
      "clusters = [] \n",
      "for i in range (0,3): # Create 3 clusters. For each cluster:\n",
      "    \n",
      "    # Fetch all document ids that belong to the current cluster number\n",
      "    clusters.append(list(key for key, value in groups if int(value) == i+1))\n",
      "    \n",
      "    # Replace the document id with the actual document    \n",
      "    clusters[i] = [final[x] for x in clusters[i]]\n",
      "    \n",
      "    clusters[i] = [doc for doc in clusters[i] if doc != 'null'] # Remove null documents \n",
      "    \n",
      "    clusters[i] = [doc for group in clusters[i] for doc in group] # Flatten list of documents into one group of words\n",
      "    \n",
      "# Create a corpus consisting of all documents\n",
      "corpus = list(abstract for cluster in clusters for abstract in cluster)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cluster_freq = [] # Empty list for storing cluster word frequencies\n",
      "\n",
      "for i in range (0,3): # For each cluster\n",
      "    cluster_freq.append(nltk.FreqDist(clusters[i])) # Compute the word frequency distrubution for the current cluster\n",
      "    \n",
      "corpus_freq = nltk.FreqDist(corpus) # Compute the word frequency distribution for the corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_sum = float(sum(corpus_freq.values())) # Compute the total number of words in the corpus\n",
      "for word in corpus_freq.keys(): # For each word in the corpus\n",
      "    corpus_freq[word] = corpus_freq[word]/corpus_sum # Calculate the probability of finding the current word in the corpus\n",
      "    \n",
      "for cluster in cluster_freq: # For each cluster\n",
      "    cluster_sum = float(sum(cluster.values())) # Compute the total number of words in the current cluster\n",
      "    for word in corpus_freq: # For each word in the corpus\n",
      "        try: # If the current word is found in the current cluster\n",
      "           cluster[word] = 0.99*(cluster[word]/cluster_sum)+0.01*corpus_freq[word] # Teleport the term using a high probability\n",
      "        except KeyError, e: # If the current word is not found in the current cluster\n",
      "           cluster[word] = 0.01*corpus_freq[word] # Teleport the term using a low probability      "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute Shannon Entropy\n",
      "shannon_entropy = [] \n",
      "for i in range (0,3):\n",
      "    shannon_entropy.append(-1.0*sum(p*math.log(p,2) for p in cluster_freq[i].values()))\n",
      "    \n",
      "# Compute cross entropies and store them in a 3x3 matrix\n",
      "cross_entropy = np.zeros((3,3))\n",
      "for i in range (0,3):\n",
      "    for j in range (0,3):\n",
      "       cross_entropy[i][j] = -1.0*sum(cluster_freq[i][word]*math.log(cluster_freq[j][word],2) for word in corpus_freq.keys()) \n",
      "        \n",
      "# Compute the efficiency of communication and jargon distances between individual clusters and store in 3x3 matrices\n",
      "efficiency = np.zeros((3,3))       \n",
      "jargon_dist = np.zeros((3,3))\n",
      "for i in range (0,3):\n",
      "    for j in range (0,3):\n",
      "        efficiency[i][j] = shannon_entropy[i]/cross_entropy[i][j]\n",
      "        jargon_dist[i][j] = 1 - efficiency[i][j]\n",
      "        if jargon_dist[i][j] < 0.001: # If jargon distance is very low, equal to zero to improve efficiency\n",
      "            jargon_dist[i][j] = 0          \n",
      "            \n",
      "print \"Time taken: \" + str(round(time.time() - start_time,2)) + \" seconds\"\n",
      "\n",
      "print \"Jargon Distances:\"\n",
      "for i in range(0,3):\n",
      "    for j in range(0,3):\n",
      "        print \"Group\", i+1, \"Group\", j+1, jargon_dist[i][j]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Time taken: 4.93 seconds\n",
        "Jargon Distances:\n",
        "Group 1 Group 1 0.0\n",
        "Group 1 Group 2 0.439177441581\n",
        "Group 1 Group 3 0.437967993467\n",
        "Group 2 Group 1 0.459090245751\n",
        "Group 2 Group 2 0.0\n",
        "Group 2 Group 3 0.46747782059\n",
        "Group 3 Group 1 0.49145440263\n",
        "Group 3 Group 2 0.510259979246\n",
        "Group 3 Group 3 0.0\n"
       ]
      }
     ],
     "prompt_number": 7
    }
   ],
   "metadata": {}
  }
 ]
}